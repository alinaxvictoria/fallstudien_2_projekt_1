---
title: "xgboost"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(mlr3)
library(mlr3learners)
library(mlr3filters)
library(mlr3pipelines)
library(mlr3tuning)
library(mlr3viz)
library(paradox)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(future)
library(future.apply)
```

```{r, warning=FALSE}
future::plan("multiprocess")
```

# xgboost Upload Rate Prediction

## Reading the Data

First, the data has to be read from the .csv file:
```{r}
data_dir = "../datasets/"

dataset_ul = read_csv(
  str_c(data_dir, "dataset_ul.csv"), 
  col_types = cols(scenario=col_factor())
)

dataset_ul_prediction = dataset_ul %>% select(
  scenario,
  velocity_mps,
  acceleration_mpss,
  rsrp_dbm,
  rsrq_db,
  rssnr_db,
  cqi,
  ta,
  payload_mb,
  f_mhz,
  throughput_mbits
)

# remove missing values
dataset_ul_prediction = dataset_ul_prediction %>% drop_na()
glimpse(dataset_ul_prediction)
```

Next, a prediction task has to be created to work with mlr3.
The goal is to predict the variable `throughput_mbits`.
```{r}
task = TaskRegr$new(
  id = "ul_prediction",
  backend = dataset_ul_prediction,
  target = "throughput_mbits"
)
task
```

## Creating the Prediction Pipeline

The next step is to create the prediction pipeline. It will consist of a factor encoding
followed by the xgboost learner. The method for factor encoding is one-hot encoding.
We put the pipeline creation inside of a function so that we can easily create
untrained copies of the pipeline later when we need them.
```{r}
make_pipeline = function() {
  factor_encoding = po(
    "encode",
    method = "one-hot",
    affect_columns = selector_type("factor")
  )
  xgboost = lrn("regr.xgboost")
  pipe = factor_encoding %>>% xgboost
  return(pipe)
}
```

Let's see what the pipeline looks like:
```{r}
pipe = make_pipeline()
pipe$plot(html=FALSE)
```

Now the pipeline has to be converted to a learner so it can be used during training and prediction:
```{r}
learner = GraphLearner$new(pipe)
```

## Parameter Tuning

First, we have to define the set of parameters we use to tune the learner:
```{r}
parameters = ParamSet$new(list(
  ParamInt$new("regr.xgboost.nrounds", lower=10, upper=500),
  ParamDbl$new("regr.xgboost.gamma", lower=0, upper=10),
  ParamInt$new("regr.xgboost.max_depth", lower=1, upper=10),
  ParamDbl$new("regr.xgboost.min_child_weight", lower=1, upper=100)
))
```

Next, we specify the tuning algorithm. For now, we use grid search:
```{r}
tuner = tnr("grid_search", resolution=10)
```

Now all that is left is putting together the parts using the AutoTuner class.
The resulting object is a learner that can automatically tune its parameters
using the algorithm we specified.
```{r}
tuned_learner = AutoTuner$new(
  learner = learner,
  resampling = rsmp("cv", folds = 5),
  measure = msr("regr.mae"),
  search_space = parameters,
  terminator = trm("evals", n_evals=100),
  tuner = tuner
)
```

## Training and Benchmarking

Create a default learner to compare it to the tuned learner:
```{r}
make_default_learner = function() {
  learner_default = GraphLearner$new(
    make_pipeline()
  )
  learner_default$param_set$values = mlr3misc::insert_named(
    learner_default$param_set$values,
    list(regr.xgboost.nrounds=100)
  )
  return(learner_default)
}
default_learner = make_default_learner()
```

```{r}
benchmark_design = benchmark_grid(
  tasks = task,
  learners = list(tuned_learner, default_learner),
  resamplings = rsmp("cv", folds=5)
)
print(benchmark_design)
```

```{r, results='hide'}
benchmark_result = benchmark(benchmark_design)
result_table = benchmark_result$aggregate(list(
  msr("regr.mse"),
  msr("regr.mae"),
  msr("regr.rsq")
))
```

```{r}
knitr::kable(result_table[, c("learner_id", "regr.mse", "regr.mae", "regr.rsq")])
```

```{r}
plot_mse = autoplot(benchmark_result, measure=msr("regr.mse")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_mae = autoplot(benchmark_result, measure=msr("regr.mae")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_rsq = autoplot(benchmark_result, measure=msr("regr.rsq")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_mse + plot_mae + plot_rsq
```

## Feature Importance

Create an untrained learner with default parameters:
```{r}
learner_default = make_default_learner()
```

```{r, results='hide'}
filter_permutation = flt("permutation",
  learner = learner_default,
  resampling = rsmp("holdout", ratio=0.8),
  measure = msr("regr.mae"),
  standardize = TRUE,
  nmc=10
)
filter_permutation$calculate(task)
```

```{r}
filter_permutation_results = as.data.table(filter_permutation)
filter_permutation_results
```

```{r}
ggplot(filter_permutation_results) +
  geom_bar(aes(x = reorder(feature, -score), y = score), stat="identity") +
  xlab("feature") +
  ylab("MAE difference") +
  scale_x_discrete(guide = guide_axis(angle = 20)) +
  ggtitle("Permutation Feature Importance")
```